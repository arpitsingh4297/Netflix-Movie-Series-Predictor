{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56fa7c99-38b5-4cf8-9f29-bb55ca9c8c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Percentage:\n",
      "show_id          0.000000\n",
      "type             0.000000\n",
      "title            0.000000\n",
      "director        29.908028\n",
      "cast             9.367549\n",
      "country          9.435676\n",
      "date_added       0.113546\n",
      "release_year     0.000000\n",
      "rating           0.045418\n",
      "duration         0.034064\n",
      "listed_in        0.000000\n",
      "description      0.000000\n",
      "dtype: float64\n",
      "\n",
      "Initial rows: 8807\n",
      "\n",
      "Correcting erroneous rating '66 min'\n",
      "\n",
      "Correcting erroneous rating '74 min'\n",
      "\n",
      "Correcting erroneous rating '84 min'\n",
      "\n",
      "Excluding exclusive ratings: ['G', 'NC-17', 'PG', 'PG-13', 'UR']\n",
      "Rows after excluding ratings: 7983\n",
      "\n",
      "Checking duration column for non-string or missing values:\n",
      "Rows with invalid duration (NaN or non-string): 0\n",
      "\n",
      "Rows with unrealistic movie durations (<15 min or >300 min): 14\n",
      "Unrealistic movie durations:\n",
      "     show_id   type duration  movie_duration_min\n",
      "71       s72  Movie   13 min                13.0\n",
      "694     s695  Movie   13 min                13.0\n",
      "695     s696  Movie   12 min                12.0\n",
      "1425   s1426  Movie   14 min                14.0\n",
      "2713   s2714  Movie    5 min                 5.0\n",
      "2858   s2859  Movie   11 min                11.0\n",
      "3364   s3365  Movie   14 min                14.0\n",
      "3535   s3536  Movie   10 min                10.0\n",
      "3775   s3776  Movie   12 min                12.0\n",
      "3777   s3778  Movie    3 min                 3.0\n",
      "4253   s4254  Movie  312 min               312.0\n",
      "4707   s4708  Movie   12 min                12.0\n",
      "5375   s5376  Movie   14 min                14.0\n",
      "6405   s6406  Movie   11 min                11.0\n",
      "Rows after dropping unrealistic durations: 7969\n",
      "Rows with unrealistic movie durations (post-drop): 0\n",
      "\n",
      "Outliers in movie_duration_min (post-capping): 0\n",
      "\n",
      "Rows with invalid date_added (NaT): 10\n",
      "Rows after dropping NaT: 7959\n",
      "\n",
      "Outliers in release_year: 696\n",
      "count     696.000000\n",
      "mean     1993.310345\n",
      "std        13.775224\n",
      "min      1925.000000\n",
      "25%      1988.000000\n",
      "50%      1998.000000\n",
      "75%      2004.000000\n",
      "max      2006.000000\n",
      "Name: release_year, dtype: float64\n",
      "\n",
      "Outliers in year_added: 50\n",
      "count      50.000000\n",
      "mean     2012.420000\n",
      "std         1.738989\n",
      "min      2008.000000\n",
      "25%      2011.000000\n",
      "50%      2013.000000\n",
      "75%      2014.000000\n",
      "max      2014.000000\n",
      "Name: year_added, dtype: float64\n",
      "\n",
      "Correlation with target (type_encoded):\n",
      "release_year: 0.0447\n",
      "year_added: 0.0687\n",
      "month_added: 0.0083\n",
      "num_genres: 0.0104\n",
      "rating_R: -0.0185\n",
      "rating_TV-14: 0.0064\n",
      "rating_TV-G: 0.0334\n",
      "rating_TV-MA: -0.0068\n",
      "rating_TV-PG: -0.0076\n",
      "rating_TV-Y: 0.0070\n",
      "rating_TV-Y7: 0.0114\n",
      "rating_TV-Y7-FV: 0.0030\n",
      "is_international_1: -0.0049\n",
      "is_drama_1: -0.0134\n",
      "is_comedy_1: 0.0003\n",
      "is_documentary_1: 0.0072\n",
      "is_action_1: 0.0099\n",
      "is_horror_1: -0.0125\n",
      "is_sci_fi_1: 0.0130\n",
      "is_romance_1: -0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\arpit\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Summary:\n",
      "                  Model      Data  Train Precision  Train Recall  Train F1  \\\n",
      "0   Logistic Regression  Original         0.666009      0.542872  0.598169   \n",
      "1   Logistic Regression  Balanced         0.681118      0.715587  0.697927   \n",
      "2           Naive Bayes  Original         0.423236      0.989818  0.592937   \n",
      "3           Naive Bayes  Balanced         0.540947      0.982794  0.697808   \n",
      "4         Decision Tree  Original         0.988466      0.918542  0.952222   \n",
      "5         Decision Tree  Balanced         0.989825      0.951754  0.970416   \n",
      "6         Random Forest  Original         0.961329      0.945874  0.953539   \n",
      "7         Random Forest  Balanced         0.971622      0.970310  0.970966   \n",
      "8              AdaBoost  Original         0.663179      0.543408  0.597349   \n",
      "9              AdaBoost  Balanced         0.717594      0.752699  0.734727   \n",
      "10              XGBoost  Original         0.884811      0.839764  0.861699   \n",
      "11              XGBoost  Balanced         0.914148      0.898111  0.906059   \n",
      "12                  SVM  Original         0.765714      0.646302  0.700959   \n",
      "13                  SVM  Balanced         0.778372      0.835358  0.805858   \n",
      "14                  KNN  Original         0.794901      0.751876  0.772790   \n",
      "15                  KNN  Balanced         0.826155      0.886640  0.855330   \n",
      "\n",
      "    Train ROC-AUC  Test Precision  Test Recall   Test F1  Test ROC-AUC  \\\n",
      "0        0.817870        0.579909     0.489403  0.530825      0.789841   \n",
      "1        0.825035        0.531627     0.680154  0.596788      0.790112   \n",
      "2        0.778147        0.409019     0.996146  0.579921      0.739322   \n",
      "3        0.778902        0.410585     0.986513  0.579841      0.735518   \n",
      "4        0.997771        0.619691     0.618497  0.619094      0.735601   \n",
      "5        0.998582        0.611429     0.618497  0.614943      0.733335   \n",
      "6        0.997159        0.672098     0.635838  0.653465      0.845864   \n",
      "7        0.998123        0.654369     0.649326  0.651838      0.844638   \n",
      "8        0.825665        0.592760     0.504817  0.545265      0.795740   \n",
      "9        0.852303        0.543307     0.664740  0.597920      0.790979   \n",
      "10       0.970182        0.678643     0.655106  0.666667      0.857686   \n",
      "11       0.978237        0.664773     0.676301  0.670487      0.861259   \n",
      "12       0.891225        0.664336     0.549133  0.601266      0.824190   \n",
      "13       0.906410        0.589147     0.732177  0.652921      0.829709   \n",
      "14       0.925355        0.631902     0.595376  0.613095      0.808017   \n",
      "15       0.947838        0.561290     0.670520  0.611062      0.801763   \n",
      "\n",
      "    Holdout Precision  Holdout Recall  Holdout F1  Holdout ROC-AUC  \\\n",
      "0            0.685590        0.558719    0.615686         0.822714   \n",
      "1            0.601770        0.725979    0.658065         0.822700   \n",
      "2            0.439750        1.000000    0.610870         0.766334   \n",
      "3            0.445161        0.982206    0.612653         0.767274   \n",
      "4            0.650376        0.615658    0.632541         0.729983   \n",
      "5            0.637037        0.612100    0.624319         0.728573   \n",
      "6            0.718876        0.637011    0.675472         0.861341   \n",
      "7            0.712644        0.661922    0.686347         0.861611   \n",
      "8            0.688312        0.565836    0.621094         0.824780   \n",
      "9            0.609375        0.693950    0.648918         0.820955   \n",
      "10           0.718147        0.661922    0.688889         0.876944   \n",
      "11           0.712644        0.661922    0.686347         0.880952   \n",
      "12           0.727273        0.597865    0.656250         0.862640   \n",
      "13           0.666667        0.775801    0.717105         0.862164   \n",
      "14           0.659004        0.612100    0.634686         0.811177   \n",
      "15           0.612179        0.679715    0.644182         0.812183   \n",
      "\n",
      "    CV F1 Mean  CV F1 Std Overfitting  \n",
      "0     0.596923   0.020579          No  \n",
      "1     0.696979   0.008329         Yes  \n",
      "2     0.592026   0.003050          No  \n",
      "3     0.698240   0.003053         Yes  \n",
      "4     0.640425   0.020698         Yes  \n",
      "5     0.768880   0.008947         Yes  \n",
      "6     0.687059   0.032160         Yes  \n",
      "7     0.810205   0.012765         Yes  \n",
      "8     0.588043   0.023546          No  \n",
      "9     0.728489   0.011249         Yes  \n",
      "10    0.700246   0.028645         Yes  \n",
      "11    0.823484   0.008908         Yes  \n",
      "12    0.660571   0.029852          No  \n",
      "13    0.778240   0.009869         Yes  \n",
      "14    0.640688   0.031100         Yes  \n",
      "15    0.777704   0.007007         Yes  \n",
      "\n",
      "Best Model: XGBoost (Balanced) with Test F1: 0.6705, Holdout F1: 0.6863\n",
      "\n",
      "Best Parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}\n",
      "\n",
      "Tuned Model Performance:\n",
      "Test Precision: 0.6486\n",
      "Test Recall: 0.6898\n",
      "Test F1: 0.6685\n",
      "Test ROC-AUC: 0.8576\n",
      "Holdout Precision: 0.7102\n",
      "Holdout Recall: 0.7153\n",
      "Holdout F1: 0.7128\n",
      "Holdout ROC-AUC: 0.8787\n",
      "Saved model as 'best_model.pkl', preprocessor as 'preprocessor.pkl', and label encoder as 'label_encoder.pkl'\n",
      "\n",
      "Key Insights:\n",
      "- Model trained and saved successfully.\n",
      "- Best Model: XGBoost (Test F1: 0.6705, Holdout F1: 0.6863).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load dataset\n",
    "def load_data():\n",
    "    import os\n",
    "    file_path = \"C:/Users/arpit/Documents/netflix_titles.csv\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found at {file_path}. Please verify the path.\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# Data Cleaning and Preprocessing\n",
    "def clean_data(data):\n",
    "    print(\"Missing Values Percentage:\")\n",
    "    print(data.isnull().mean() * 100)\n",
    "    \n",
    "    print(f\"\\nInitial rows: {len(data)}\")\n",
    "    \n",
    "    # Fix erroneous ratings\n",
    "    erroneous_ratings = ['66 min', '74 min', '84 min']\n",
    "    for rating in erroneous_ratings:\n",
    "        if rating in data['rating'].values:\n",
    "            print(f\"\\nCorrecting erroneous rating '{rating}'\")\n",
    "            data.loc[data['rating'] == rating, 'duration'] = rating\n",
    "            data.loc[data['rating'] == rating, 'rating'] = data['rating'].mode()[0]\n",
    "    \n",
    "    # Exclude exclusive ratings\n",
    "    exclusive_ratings = ['G', 'NC-17', 'PG', 'PG-13', 'UR']\n",
    "    print(f\"\\nExcluding exclusive ratings: {exclusive_ratings}\")\n",
    "    data = data[~data['rating'].isin(exclusive_ratings)]\n",
    "    print(f\"Rows after excluding ratings: {len(data)}\")\n",
    "    \n",
    "    # Impute missing values for categorical columns\n",
    "    cat_cols = ['director', 'cast', 'country', 'rating']\n",
    "    for col in cat_cols:\n",
    "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "    \n",
    "    # Handle missing duration values\n",
    "    print(\"\\nChecking duration column for non-string or missing values:\")\n",
    "    invalid_duration = data[data['duration'].isna() | ~data['duration'].apply(lambda x: isinstance(x, str))]\n",
    "    print(f\"Rows with invalid duration (NaN or non-string): {len(invalid_duration)}\")\n",
    "    if len(invalid_duration) > 0:\n",
    "        print(\"Invalid duration values:\")\n",
    "        print(invalid_duration[['show_id', 'type', 'duration']])\n",
    "    \n",
    "    data['duration'] = data.apply(\n",
    "        lambda x: \"90 min\" if pd.isna(x['duration']) and x['type'] == 'Movie' \n",
    "        else \"1 Season\" if pd.isna(x['duration']) and x['type'] == 'TV Show' \n",
    "        else x['duration'], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Validate movie durations\n",
    "    data['movie_duration_min'] = data.apply(\n",
    "        lambda x: float(x['duration'].split()[0]) if isinstance(x['duration'], str) and 'min' in x['duration'] and x['type'] == 'Movie' else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    invalid_movies = data[(data['type'] == 'Movie') & (data['movie_duration_min'] > 0) & ((data['movie_duration_min'] < 15) | (data['movie_duration_min'] > 300))]\n",
    "    print(f\"\\nRows with unrealistic movie durations (<15 min or >300 min): {len(invalid_movies)}\")\n",
    "    if len(invalid_movies) > 0:\n",
    "        print(\"Unrealistic movie durations:\")\n",
    "        print(invalid_movies[['show_id', 'type', 'duration', 'movie_duration_min']])\n",
    "    \n",
    "    # Drop unrealistic durations\n",
    "    data = data[~((data['type'] == 'Movie') & (data['movie_duration_min'] > 0) & ((data['movie_duration_min'] < 15) | (data['movie_duration_min'] > 300)))]\n",
    "    print(f\"Rows after dropping unrealistic durations: {len(data)}\")\n",
    "    \n",
    "    # Validate post-drop\n",
    "    invalid_movies_post_drop = data[(data['type'] == 'Movie') & (data['movie_duration_min'] > 0) & ((data['movie_duration_min'] < 15) | (data['movie_duration_min'] > 300))]\n",
    "    print(f\"Rows with unrealistic movie durations (post-drop): {len(invalid_movies_post_drop)}\")\n",
    "    \n",
    "    # Cap outliers in movie_duration_min\n",
    "    Q1 = data[data['movie_duration_min'] > 0]['movie_duration_min'].quantile(0.25)\n",
    "    Q3 = data[data['movie_duration_min'] > 0]['movie_duration_min'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = max(15, Q1 - 1.5 * IQR)\n",
    "    upper_bound = min(300, Q3 + 1.5 * IQR)\n",
    "    data.loc[(data['type'] == 'Movie') & (data['movie_duration_min'] > 0) & (data['movie_duration_min'] < lower_bound), 'movie_duration_min'] = lower_bound\n",
    "    data.loc[(data['type'] == 'Movie') & (data['movie_duration_min'] > 0) & (data['movie_duration_min'] > upper_bound), 'movie_duration_min'] = upper_bound\n",
    "    \n",
    "    # Recompute outliers\n",
    "    duration_outliers = data[(data['movie_duration_min'] > 0) & ((data['movie_duration_min'] < lower_bound) | (data['movie_duration_min'] > upper_bound))]['movie_duration_min']\n",
    "    print(f\"\\nOutliers in movie_duration_min (post-capping): {len(duration_outliers)}\")\n",
    "    \n",
    "    # Impute zeros with median\n",
    "    movie_median = data[data['movie_duration_min'] > 0]['movie_duration_min'].median()\n",
    "    data['movie_duration_min'] = data['movie_duration_min'].replace(0, movie_median)\n",
    "    \n",
    "    # Clean date_added\n",
    "    data['date_added'] = data['date_added'].str.strip()\n",
    "    data['date_added'] = pd.to_datetime(data['date_added'], errors='coerce', format='mixed')\n",
    "    print(f\"\\nRows with invalid date_added (NaT): {data['date_added'].isna().sum()}\")\n",
    "    data.dropna(subset=['date_added'], inplace=True)\n",
    "    print(f\"Rows after dropping NaT: {len(data)}\")\n",
    "    \n",
    "    # Extract year and month\n",
    "    data['year_added'] = data['date_added'].dt.year\n",
    "    data['month_added'] = data['date_added'].dt.month\n",
    "    \n",
    "    # Outlier detection\n",
    "    numerical_cols = ['release_year', 'year_added']\n",
    "    for col in numerical_cols:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)][col]\n",
    "        print(f\"\\nOutliers in {col}: {len(outliers)}\")\n",
    "        print(outliers.describe())\n",
    "        if col == 'year_added':\n",
    "            data[col] = data[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Feature Engineering and Selection\n",
    "def feature_engineering(data):\n",
    "    global preprocessor, le  # Make preprocessor and label encoder global for saving\n",
    "    le = LabelEncoder()\n",
    "    data['type_encoded'] = le.fit_transform(data['type'])\n",
    "    \n",
    "    # Genre features\n",
    "    data['num_genres'] = data['listed_in'].apply(lambda x: len(x.split(',')))\n",
    "    data['is_international'] = data['country'].apply(lambda x: 1 if 'United States' not in x else 0)\n",
    "    data['is_drama'] = data['listed_in'].apply(lambda x: 1 if 'Dramas' in x else 0)\n",
    "    data['is_comedy'] = data['listed_in'].apply(lambda x: 1 if 'Comedies' in x else 0)\n",
    "    data['is_documentary'] = data['listed_in'].apply(lambda x: 1 if 'Documentaries' in x else 0)\n",
    "    data['is_action'] = data['listed_in'].apply(lambda x: 1 if 'Action' in x else 0)\n",
    "    data['is_horror'] = data['listed_in'].apply(lambda x: 1 if 'Horror' in x else 0)\n",
    "    data['is_sci_fi'] = data['listed_in'].apply(lambda x: 1 if 'Sci-Fi' in x else 0)\n",
    "    data['is_romance'] = data['listed_in'].apply(lambda x: 1 if 'Romantic' in x else 0)\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    numerical_features = ['release_year', 'year_added', 'month_added', 'num_genres']\n",
    "    categorical_features = ['rating', 'is_international', 'is_drama', 'is_comedy', 'is_documentary', 'is_action', 'is_horror', 'is_sci_fi', 'is_romance']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    X = preprocessor.fit_transform(data)\n",
    "    y = data['type_encoded']\n",
    "    \n",
    "    # Get feature names\n",
    "    num_features = numerical_features\n",
    "    cat_features = []\n",
    "    for feature in categorical_features:\n",
    "        if feature == 'rating':\n",
    "            cat_features.extend([f\"rating_{cat}\" for cat in preprocessor.named_transformers_['cat'].categories_[0][1:]])\n",
    "        else:\n",
    "            cat_features.extend([f\"{feature}_{cat}\" for cat in preprocessor.named_transformers_['cat'].categories_[categorical_features.index(feature)][1:]])\n",
    "    features = num_features + cat_features\n",
    "    \n",
    "    # Check correlations\n",
    "    print(\"\\nCorrelation with target (type_encoded):\")\n",
    "    X_df = pd.DataFrame(X, columns=features)\n",
    "    for feature in features:\n",
    "        corr = X_df[feature].corr(y)\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "        if abs(corr) > 0.8:\n",
    "            print(f\"Warning: High correlation ({feature}) may indicate leakage.\")\n",
    "    \n",
    "    return X, y, features\n",
    "\n",
    "# Model Building and Evaluation\n",
    "def build_models(X, y, features):\n",
    "    # Create holdout set\n",
    "    X_temp, X_holdout, y_temp, y_holdout = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=0.2222, random_state=42)\n",
    "    \n",
    "    # Try SMOTE with sampling_strategy=0.8, fallback to 1.0 if it fails\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42, sampling_strategy=0.8)\n",
    "        X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "    except ValueError as e:\n",
    "        print(f\"SMOTE failed with sampling_strategy=0.8: {e}\")\n",
    "        print(\"Retrying with sampling_strategy=1.0\")\n",
    "        smote = SMOTE(random_state=42, sampling_strategy=1.0)\n",
    "        X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier()\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_pred_holdout = model.predict(X_holdout)\n",
    "        train_probs = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_train)\n",
    "        test_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "        holdout_probs = model.predict_proba(X_holdout)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_holdout)\n",
    "        \n",
    "        train_metrics = {\n",
    "            'Precision': precision_score(y_train, y_pred_train),\n",
    "            'Recall': recall_score(y_train, y_pred_train),\n",
    "            'F1': f1_score(y_train, y_pred_train),\n",
    "            'ROC-AUC': roc_auc_score(y_train, train_probs)\n",
    "        }\n",
    "        test_metrics = {\n",
    "            'Precision': precision_score(y_test, y_pred_test),\n",
    "            'Recall': recall_score(y_test, y_pred_test),\n",
    "            'F1': f1_score(y_test, y_pred_test),\n",
    "            'ROC-AUC': roc_auc_score(y_test, test_probs),\n",
    "            'CV F1 Mean': cv_scores.mean(),\n",
    "            'CV F1 Std': cv_scores.std()\n",
    "        }\n",
    "        holdout_metrics = {\n",
    "            'Precision': precision_score(y_holdout, y_pred_holdout),\n",
    "            'Recall': recall_score(y_holdout, y_pred_holdout),\n",
    "            'F1': f1_score(y_holdout, y_pred_holdout),\n",
    "            'ROC-AUC': roc_auc_score(y_holdout, holdout_probs)\n",
    "        }\n",
    "        \n",
    "        cv_scores_bal = cross_val_score(model, X_train_bal, y_train_bal, cv=cv, scoring='f1')\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "        y_pred_train_bal = model.predict(X_train_bal)\n",
    "        y_pred_test_bal = model.predict(X_test)\n",
    "        y_pred_holdout_bal = model.predict(X_holdout)\n",
    "        train_probs_bal = model.predict_proba(X_train_bal)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_train_bal)\n",
    "        test_probs_bal = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "        holdout_probs_bal = model.predict_proba(X_holdout)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_holdout)\n",
    "        \n",
    "        train_metrics_bal = {\n",
    "            'Precision': precision_score(y_train_bal, y_pred_train_bal),\n",
    "            'Recall': recall_score(y_train_bal, y_pred_train_bal),\n",
    "            'F1': f1_score(y_train_bal, y_pred_train_bal),\n",
    "            'ROC-AUC': roc_auc_score(y_train_bal, train_probs_bal)\n",
    "        }\n",
    "        test_metrics_bal = {\n",
    "            'Precision': precision_score(y_test, y_pred_test_bal),\n",
    "            'Recall': recall_score(y_test, y_pred_test_bal),\n",
    "            'F1': f1_score(y_test, y_pred_test_bal),\n",
    "            'ROC-AUC': roc_auc_score(y_test, test_probs_bal),\n",
    "            'CV F1 Mean': cv_scores_bal.mean(),\n",
    "            'CV F1 Std': cv_scores_bal.std()\n",
    "        }\n",
    "        holdout_metrics_bal = {\n",
    "            'Precision': precision_score(y_holdout, y_pred_holdout_bal),\n",
    "            'Recall': recall_score(y_holdout, y_pred_holdout_bal),\n",
    "            'F1': f1_score(y_holdout, y_pred_holdout_bal),\n",
    "            'ROC-AUC': roc_auc_score(y_holdout, holdout_probs_bal)\n",
    "        }\n",
    "        \n",
    "        overfitting = 'Yes' if (train_metrics['F1'] - test_metrics['F1'] > 0.1) else 'No'\n",
    "        overfitting_bal = 'Yes' if (train_metrics_bal['F1'] - test_metrics_bal['F1'] > 0.1) else 'No'\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Data': 'Original',\n",
    "            'Train Precision': train_metrics['Precision'],\n",
    "            'Train Recall': train_metrics['Recall'],\n",
    "            'Train F1': train_metrics['F1'],\n",
    "            'Train ROC-AUC': train_metrics['ROC-AUC'],\n",
    "            'Test Precision': test_metrics['Precision'],\n",
    "            'Test Recall': test_metrics['Recall'],\n",
    "            'Test F1': test_metrics['F1'],\n",
    "            'Test ROC-AUC': test_metrics['ROC-AUC'],\n",
    "            'Holdout Precision': holdout_metrics['Precision'],\n",
    "            'Holdout Recall': holdout_metrics['Recall'],\n",
    "            'Holdout F1': holdout_metrics['F1'],\n",
    "            'Holdout ROC-AUC': holdout_metrics['ROC-AUC'],\n",
    "            'CV F1 Mean': test_metrics['CV F1 Mean'],\n",
    "            'CV F1 Std': test_metrics['CV F1 Std'],\n",
    "            'Overfitting': overfitting\n",
    "        })\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Data': 'Balanced',\n",
    "            'Train Precision': train_metrics_bal['Precision'],\n",
    "            'Train Recall': train_metrics_bal['Recall'],\n",
    "            'Train F1': train_metrics_bal['F1'],\n",
    "            'Train ROC-AUC': train_metrics_bal['ROC-AUC'],\n",
    "            'Test Precision': test_metrics_bal['Precision'],\n",
    "            'Test Recall': test_metrics_bal['Recall'],\n",
    "            'Test F1': test_metrics_bal['F1'],\n",
    "            'Test ROC-AUC': test_metrics_bal['ROC-AUC'],\n",
    "            'Holdout Precision': holdout_metrics_bal['Precision'],\n",
    "            'Holdout Recall': holdout_metrics_bal['Recall'],\n",
    "            'Holdout F1': holdout_metrics_bal['F1'],\n",
    "            'Holdout ROC-AUC': holdout_metrics_bal['ROC-AUC'],\n",
    "            'CV F1 Mean': test_metrics_bal['CV F1 Mean'],\n",
    "            'CV F1 Std': test_metrics_bal['CV F1 Std'],\n",
    "            'Overfitting': overfitting_bal\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    best_model_row = results_df.loc[results_df['Test F1'].idxmax()]\n",
    "    print(f\"\\nBest Model: {best_model_row['Model']} ({best_model_row['Data']}) with Test F1: {best_model_row['Test F1']:.4f}, Holdout F1: {best_model_row['Holdout F1']:.4f}\")\n",
    "    \n",
    "    return results_df, best_model_row, X_train, X_test, y_train, y_test, X_train_bal, y_train_bal, X_holdout, y_holdout\n",
    "\n",
    "# Model Tuning\n",
    "def tune_model(best_model_row, X_train, X_test, y_train, y_test, X_train_bal, y_train_bal, X_holdout, y_holdout):\n",
    "    model_name = best_model_row['Model']\n",
    "    data_type = best_model_row['Data']\n",
    "    \n",
    "    param_grid = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']},\n",
    "        'Naive Bayes': {},\n",
    "        'Decision Tree': {'max_depth': [5, 10, None], 'min_samples_split': [2, 5, 10]},\n",
    "        'Random Forest': {'n_estimators': [100, 200], 'max_depth': [10, 20, None]},\n",
    "        'AdaBoost': {'n_estimators': [50, 100], 'learning_rate': [0.1, 1.0]},\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.3]\n",
    "        },\n",
    "        'SVM': {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']},\n",
    "        'KNN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "    }\n",
    "    \n",
    "    model_dict = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier()\n",
    "    }\n",
    "    \n",
    "    model = model_dict[model_name]\n",
    "    \n",
    "    if param_grid[model_name]:\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(model, param_grid[model_name], cv=cv, scoring='recall', n_jobs=-1)\n",
    "        if data_type == 'Balanced':\n",
    "            grid_search.fit(X_train_bal, y_train_bal)\n",
    "        else:\n",
    "            grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"\\nBest Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        model = grid_search.best_estimator_\n",
    "    \n",
    "    if data_type == 'Balanced':\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_holdout = model.predict(X_holdout)\n",
    "    test_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    holdout_probs = model.predict_proba(X_holdout)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_holdout)\n",
    "    \n",
    "    print(\"\\nTuned Model Performance:\")\n",
    "    print(f\"Test Precision: {precision_score(y_test, y_pred_test):.4f}\")\n",
    "    print(f\"Test Recall: {recall_score(y_test, y_pred_test):.4f}\")\n",
    "    print(f\"Test F1: {f1_score(y_test, y_pred_test):.4f}\")\n",
    "    print(f\"Test ROC-AUC: {roc_auc_score(y_test, test_probs):.4f}\")\n",
    "    print(f\"Holdout Precision: {precision_score(y_holdout, y_pred_holdout):.4f}\")\n",
    "    print(f\"Holdout Recall: {recall_score(y_holdout, y_pred_holdout):.4f}\")\n",
    "    print(f\"Holdout F1: {f1_score(y_holdout, y_pred_holdout):.4f}\")\n",
    "    print(f\"Holdout ROC-AUC: {roc_auc_score(y_holdout, holdout_probs):.4f}\")\n",
    "    \n",
    "    # Save model, preprocessor, and label encoder\n",
    "    joblib.dump(model, 'best_model.pkl')\n",
    "    joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "    joblib.dump(le, 'label_encoder.pkl')\n",
    "    print(\"Saved model as 'best_model.pkl', preprocessor as 'preprocessor.pkl', and label encoder as 'label_encoder.pkl'\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data = load_data()\n",
    "        data = clean_data(data)\n",
    "        X, y, features = feature_engineering(data)\n",
    "        results_df, best_model_row, X_train, X_test, y_train, y_test, X_train_bal, y_train_bal, X_holdout, y_holdout = build_models(X, y, features)\n",
    "        tuned_model = tune_model(best_model_row, X_train, X_test, y_train, y_test, X_train_bal, y_train_bal, X_holdout, y_holdout)\n",
    "        \n",
    "        print(\"\\nKey Insights:\")\n",
    "        print(\"- Model trained and saved successfully.\")\n",
    "        print(f\"- Best Model: {best_model_row['Model']} (Test F1: {best_model_row['Test F1']:.4f}, Holdout F1: {best_model_row['Holdout F1']:.4f}).\")\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd084fd2-bc6e-4412-bd32-6495a49da2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
